# CM3: Word Embeddings

[Slides used (PDF)](https://drive.google.com/file/d/1f7vQGNRe1PQi6WnEdCZbtf_zmx6fF8g1/view)

**Topics:**
- Summary of approaches to vector representation.
- Negative sampling.
- Word2Vec: skip-gram vs CBOW.
- Linear operations with vectors, including addition and subtraction.
- Impact of large/small context window size on embedding results.
- Problem statement for text classification.
- Overview of feature extraction approaches: count-based vs neural.
- Overview of text classification with Naive Bayes.

![Embedding Space Walk](https://github.com/yandexdataschool/nlp_course/blob/2024/resources/nlp2020_gifs/walk_through_space.gif)

Link to try yourself: [LINK](https://lena-voita.github.io/nlp_course/word_embeddings.html#analysis_interpretability).

Text classification (theory): [Speech and Language Processing - Chapter 4](https://web.stanford.edu/~jurafsky/slp3/4.pdf)
