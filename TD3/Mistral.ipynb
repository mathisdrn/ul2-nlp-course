{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bae70d-0b59-43eb-a47a-bf71a002623b",
   "metadata": {},
   "source": [
    "# Mise en oeuvre du modèle Mistral Large (123B paramètres ?)\n",
    "\n",
    "Julien Velcin, Université Lyon 2 - Master Humanités Numériques\n",
    "\n",
    "Plus de détails sur le modèle : https://mistral.ai/fr/news/mistral-large/\n",
    "\n",
    "La partie RAG suit le traitement proposé ici : https://docs.mistral.ai/guides/rag/\n",
    "\n",
    "Il est possible d'affiner (fine tune) ce type de modèle, y compris via l'API si on n'a pas assez de ressources de calcul en local, mais ce ne sera pas vu dans ce cours : https://docs.mistral.ai/guides/finetuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71de07-ff3d-4da4-b056-3113040d17d9",
   "metadata": {},
   "source": [
    "Ce qui suit est une démonstration de l'utilisation de l'API de Mistral, comme alternative aux modèles de référence comme GPT. A noter qu'il est possible, en plus de faire de la génération à partir de prompt et des données de préentrainement du modèle, de suivre le principe RAG (Retrieval Augmented Generation) afin de coupler ça avec une base de données qui a été indexée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a86e29f-5e89-4f62-8bea-d8ec6093ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairies utilisées\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import faiss\n",
    "import pickle\n",
    "import faiss.contrib.torch_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf64b44-9115-47c1-a505-437ae410ddfe",
   "metadata": {},
   "source": [
    "Tout d'abord, il s'agit de charger la clef d'accès à l'API de Mistral (gratuit pour une utilisation individuelle mais potentiellement limitée)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc85de4-23c7-47cc-9b26-d46016dcd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"MISTRAL_KEY_API\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fef28-a4cc-4687-b3a6-b5f178b488b9",
   "metadata": {},
   "source": [
    "Après avoir [installé la librairie *mistralai*](https://docs.mistral.ai/getting-started/clients/), vous pouvez la charger en mémoire et choisir votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a11d85-df8c-4806-89a2-cbf9153a50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "# choix du modèle (ici la version large)\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61105d84-a57e-4c99-a4ef-1787eb40e5d8",
   "metadata": {},
   "source": [
    "Pour commencer, quelques tests simples d'autocomplétion, en anglais..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38054bc1-4921-481e-9fcc-f9ad916e5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing the \"best\" French cheese can be subjective, as it depends on personal taste. France is renowned for its wide variety of cheeses, with over 400 different types. Here are a few highly regarded French cheeses across various categories:\n",
      "\n",
      "1. **Soft Cheeses**:\n",
      "   - **Brie de Meaux**: Known for its creamy texture and rich, slightly nutty flavor.\n",
      "   - **Camembert de Normandie**: Soft and creamy with a strong, earthy aroma.\n",
      "\n",
      "2. **Semi-Soft Cheeses**:\n",
      "   - **Morbier**: Recognizable by its layer of ash in the middle, it has a fruity and slightly nutty taste.\n",
      "   - **Reblochon**: A nutty and fruity cheese from the Alps, often used in tartiflette.\n",
      "\n",
      "3. **Hard Cheeses**:\n",
      "   - **Comté**: A firm, nutty, and slightly sweet cheese made from unpasteurized cow's milk.\n",
      "   - **Beaufort**: Similar to Comté, it has a firm texture and a complex, nutty flavor.\n",
      "\n",
      "4. **Blue Cheeses**:\n",
      "   - **Roquefort**: A tangy and salty sheep milk cheese with distinctive veins of blue mold.\n",
      "   - **Bleu d'Auvergne**: A strong, pungent cow's milk blue cheese with a moist, crumbly texture.\n",
      "\n",
      "5. **Goat Cheeses**:\n",
      "   - **Chèvre (Goat Cheese)**: Available in many forms, from fresh and soft to aged and crumbly, often with a tangy flavor.\n",
      "   - **Crottin de Chavignol**: A small, circular goat cheese that can be soft and mild when young, or firm and nutty when aged.\n",
      "\n",
      "Each of these cheeses has its unique characteristics, so the \"best\" one depends on your preferences. It's always fun to try a variety to discover your favorite!\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.complete(\n",
    "    model= model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", # \"rôle\" que va jouer le robot (liste à choix fermée)\n",
    "            \"content\": \"What is the best French cheese?\", # la question\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ed256-b321-49a5-be6b-3e6f82014f1f",
   "metadata": {},
   "source": [
    "...et en français :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b15cca-b672-4938-86c4-4899e7ee006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La question du \"meilleur fromage\" est très subjective et dépend des goûts personnels de chacun. La France, par exemple, est célèbre pour sa grande variété de fromages, chacun ayant ses propres caractéristiques et saveurs. Voici quelques fromages français très appréciés :\n",
      "\n",
      "1. **Camembert** : Un fromage à pâte molle et à croûte fleurie, originaire de Normandie.\n",
      "2. **Roquefort** : Un fromage persillé (à pâte bleue) fabriqué à partir de lait de brebis, originaire de l'Aveyron.\n",
      "3. **Brie de Meaux** : Un fromage à pâte molle et à croûte fleurie, souvent considéré comme le \"roi des fromages\".\n",
      "4. **Comté** : Un fromage à pâte pressée cuite, originaire du Jura et de la Franche-Comté.\n",
      "5. **Reblochon** : Un fromage à pâte pressée non cuite, originaire de la Savoie et de la Haute-Savoie.\n",
      "6. **Chèvre** : Il existe de nombreuses variétés de fromages de chèvre, comme le Crottin de Chavignol ou le Sainte-Maure de Touraine.\n",
      "\n",
      "Chacun de ces fromages a ses propres caractéristiques et peut être apprécié de différentes manières. Le \"meilleur\" fromage est donc celui que vous préférez personnellement !\n"
     ]
    }
   ],
   "source": [
    "chat_response_fr = client.chat.complete(\n",
    "    model= model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"La réponse s'adresse à un locuteur français. Quel est le meilleur fromage ?\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(chat_response_fr.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a791b5-e212-4d2c-b66c-de8b33fa5090",
   "metadata": {},
   "source": [
    "On peut aussi demander à un modèle proche de fournir les plongements (*embeddings*) de textes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f959bc2-6763-4c75-ae22-4f7843f3e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = \"mistral-embed\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "embeddings_response = client.embeddings.create(\n",
    "    model=model_emb,\n",
    "    inputs=[\"Un fromage à pâte pressée cuite, originaire du Jura et de la Franche-Comté.\",\n",
    "            \"Il existe de nombreuses variétés de fromages de chèvre, comme le Crottin de Chavignol ou le Sainte-Maure de Tourain.\",\n",
    "           \"Le pithiviers est un gâteau français à base de pâte feuilletée originaire de la commune de Pithiviers située dans le département du Loiret et la région Centre-Val de Loire.\",\n",
    "           \"Le château de Versailles est un château et un monument historique français situé à Versailles, dans les Yvelines.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ef6a878-8d96-4548-b3f6-21e7beceb6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "vectors = [np.array(e.embedding) for e in embeddings_response.data]\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "256016c9-81cc-46fb-9d0c-a440094b2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarité cosinus entre deux vecteurs sous numpy\n",
    "def cosine(x, y):\n",
    "    return float(cosine_similarity(x.reshape(1, -1), y.reshape(1, -1)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58722edb-6fe2-4ec1-868b-87b5124bf615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité cosinus : 0.830 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarité cosinus : {cosine(vectors[0], vectors[1]):.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a05f3-bcd4-4164-9ebe-355ca0b93329",
   "metadata": {},
   "source": [
    "# Combiner Mistral avec du RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1973e143-650b-4a52-bb52-b104910aac45",
   "metadata": {},
   "source": [
    "Le RAG (*Retrieval Augmented Generation*) permet de combiner la recherche d'information (pour trouver les meilleurs passages) et la génération d'une réponse.\n",
    "\n",
    "La solution présentée ici ne recourt pas à des librairies comme Langchain ou LlamaIndex, que nous ne verrons pas en cours, mais beaucoup de ressources en ligne en parlent très bien (comme : https://docs.mistral.ai/guides/rag/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756af9a5-3beb-471a-aab9-85da39b692eb",
   "metadata": {},
   "source": [
    "Chargement des données textuelles à partir d'un fichier (cf. début du cours) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5801ce8c-1141-45fe-8475-bc9dbdebcc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5777 lignes dans le fichier texte\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"data\", \"alice.txt\")) as f:    \n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"{len(lines)} lignes dans le fichier texte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffd833-ecc7-4f73-b601-e2ada56b294b",
   "metadata": {},
   "source": [
    "On combine les lignes pour produire un corpus de paragraphes (il faut éviter les documents vides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbee879e-6c04-46b3-b292-ee25d71ca567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033 paragraphes\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "s = \"\"\n",
    "for l in lines:\n",
    "    if (l != \"\"):\n",
    "        s = s + \" \" + l\n",
    "    else:\n",
    "        if (s != \"\"):\n",
    "            docs = docs + [s]\n",
    "            s = \"\"\n",
    "\n",
    "print(f\"{len(docs)} paragraphes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ad50a-47c1-40a0-88a5-387e27079535",
   "metadata": {},
   "source": [
    "On choisit le modèle d'embedding et on initialise le modèle (comme tout à l'heure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd89c98-c49c-408f-aff9-fcbf1930d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = \"mistral-embed\"\n",
    "\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bac49-ee24-4c50-9479-0cc3a4abe65b",
   "metadata": {},
   "source": [
    "Si la base de données vectorielle n'a pas été créée, il faut le faire (cela peut prendre du temps). Attribuer alors valeur *True* à la variable *build_embeddings* pour lancer le travail avec l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1170e5ef-2987-4d75-a601-4625883fec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n",
      "512\n",
      "497\n"
     ]
    }
   ],
   "source": [
    "build_embeddings = False # mettre True pour recalculer tous les plongements\n",
    "\n",
    "if build_embeddings:\n",
    "\n",
    "    text_embeddings = []\n",
    "    docs_2processed = docs.copy()\n",
    "    \n",
    "    step = 512 # nombre de documents traités à la fois (à priori, max 512 pour Mistral large)\n",
    "\n",
    "    while (len(docs_2processed)>step):\n",
    "        # liste des documents à traiter\n",
    "        liste_docs = docs_2processed[:step]\n",
    "\n",
    "        #on demande à l'API de calculer les vecteurs\n",
    "        embeddings_response = client.embeddings.create(\n",
    "            model=model_emb,\n",
    "            inputs = liste_docs\n",
    "        )\n",
    "\n",
    "        # on ajoute à la liste des vecteurs déjà calculés les nouveaux plongeements\n",
    "        text_embeddings.append([np.array(e.embedding) for e in embeddings_response.data])\n",
    "\n",
    "        # on décale le \"curseur\" sur la liste de traitement\n",
    "        docs_2processed = docs_2processed[step:]\n",
    "        print(len(liste_docs))\n",
    "\n",
    "        # petit temps de latence pour ne pas surcharger l'API\n",
    "        time.sleep(5)\n",
    "\n",
    "    # même traitement mais pour les derniers documents (liste de longueur < step)\n",
    "    if (len(docs_2processed)>0):\n",
    "        liste_docs = docs_2processed\n",
    "        embeddings_response = client.embeddings.create(\n",
    "            model=model_emb,\n",
    "            inputs = liste_docs\n",
    "        )\n",
    "        print(len(liste_docs))\n",
    "        text_embeddings.append([np.array(e.embedding) for e in embeddings_response.data])\n",
    "\n",
    "    # On construit une liste de tableau format numpy à partir de la liste des tableaux (c'est une simple concaténation)\n",
    "    vectors_list = np.array([j for i in text_embeddings for j in i] )\n",
    "\n",
    "    # On pense à sauvegarder le tableau de plongement dans un fichier sur le disque\n",
    "    with open(\"save_emb_alice.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vectors_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dcedd74-1220-426f-bb45-830855e370ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not build_embeddings:\n",
    "with open(\"save_emb_alice.pkl\", \"rb\") as f:\n",
    "    vectors_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80c80b1-c8d5-4400-b2ad-c95d1ca87b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2033, 1024)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d962e-1cf5-47ba-9851-f1c20d4920fa",
   "metadata": {},
   "source": [
    "A partir de là, on peut créer l'index de la base de données vectorielle grâce à la libaririe *faiss* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8b8ec1f-5ef6-4da7-a033-007713794bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "d = vectors_list_f32.shape[1]\n",
    "print(d)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(index.is_trained)\n",
    "index.add(vectors_list_f32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1aa21f-f5cf-4d76-8fcf-ddfad2b9697a",
   "metadata": {},
   "source": [
    "Note : pour accélérer les choses, le mieux serait de sauvegarder cet index sur le disque et pas de le recalculer à chaque fois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5279a89-3d49-431d-b52c-01e5a56d517f",
   "metadata": {},
   "source": [
    "# Poser des questions sur les données textuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee54a53-b7f9-479a-bcfb-7da6ba2ce9ad",
   "metadata": {},
   "source": [
    "A présent, on peut utiliser l'index pour poser n'importe quelle question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "147896da-2e1d-4f41-8cb6-38edbf5205b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poser_une_question(question):\n",
    "\n",
    "    # création du plongement (=vecteur) correspondant à la question posée\n",
    "    embeddings_response = client.embeddings.create(\n",
    "        model=model_emb,\n",
    "        inputs = question\n",
    "    )\n",
    "    question_embedding = np.array([embeddings_response.data[0].embedding])\n",
    "\n",
    "    # recherche d'information : on sélectionne les 2 documents ayant des vecteurs les plus similaires à la question (cf. TD1)\n",
    "    D, I = index.search(question_embedding, k=2)\n",
    "    retrieved_docs = [docs[i] for i in I.tolist()[0]]\n",
    "\n",
    "    # on crée l'invite (prompt) qui va nous permettre d'interroger le modèle de langue\n",
    "    prompt = f\"\"\"\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "            {retrieved_docs}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "            Query: {question}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    # ajout d'un petit temps de latence pour éviter de surcharger l'API\n",
    "    time.sleep(5)\n",
    "\n",
    "    # on lance la requête en ligne\n",
    "    chat_response = client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c38591c-32a8-4271-ac75-febcdd84b3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duchess comes out carrying a pig in baby's clothes.\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What unexpected animal the duchess comes out carrying in baby's clothes?\"\n",
    "question2 = \"What is the usual fate reserved for the queen's enemies?\"\n",
    "\n",
    "print(poser_une_question(question1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64627427-0149-4aca-b960-775bec995c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The usual fate reserved for the queen's enemies is to be beheaded.\n"
     ]
    }
   ],
   "source": [
    "print(poser_une_question(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b1d62-6817-4f2b-b528-59547b67c92f",
   "metadata": {},
   "source": [
    "Les réponses apportée semblent tout à fait pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09819b-83e5-4611-b665-01e30b629c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
