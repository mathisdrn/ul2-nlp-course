{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bae70d-0b59-43eb-a47a-bf71a002623b",
   "metadata": {},
   "source": [
    "# Mise en oeuvre du modèle Mistral Large (123B paramètres ?)\n",
    "\n",
    "Plus de détails sur le modèle : https://mistral.ai/fr/news/mistral-large/\n",
    "\n",
    "La partie RAG suit le traitement proposé ici : https://docs.mistral.ai/guides/rag/\n",
    "\n",
    "Il est possible d'affiner (fine tune) ce type de modèle, y compris via l'API si on n'a pas assez de ressources de calcul en local, mais ce ne sera pas vu dans ce cours : https://docs.mistral.ai/guides/finetuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71de07-ff3d-4da4-b056-3113040d17d9",
   "metadata": {},
   "source": [
    "Ce qui suit est une démonstration de l'utilisation de l'API de Mistral, comme alternative aux modèles de référence comme GPT. A noter qu'il est possible, en plus de faire de la génération à partir de prompt et des données de préentrainement du modèle, de suivre le principe RAG (Retrieval Augmented Generation) afin de coupler ça avec une base de données qui a été indexée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a86e29f-5e89-4f62-8bea-d8ec6093ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairies utilisées\n",
    "# !pip install mistralai faiss-gpu # if you use GPU\n",
    "# !pip install mistralai faiss-cpu # if you use CPU\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf64b44-9115-47c1-a505-437ae410ddfe",
   "metadata": {},
   "source": [
    "Tout d'abord, il s'agit de charger la clef d'accès à l'API de Mistral (gratuit pour une utilisation individuelle mais potentiellement limitée)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e8f28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_KEY_API\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fef28-a4cc-4687-b3a6-b5f178b488b9",
   "metadata": {},
   "source": [
    "Après avoir [installé la librairie *mistralai*](https://docs.mistral.ai/getting-started/clients/), vous pouvez la charger en mémoire et choisir votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19a11d85-df8c-4806-89a2-cbf9153a50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "# choix du modèle (ici la version large)\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61105d84-a57e-4c99-a4ef-1787eb40e5d8",
   "metadata": {},
   "source": [
    "Pour commencer, quelques tests simples d'autocomplétion, en anglais..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38054bc1-4921-481e-9fcc-f9ad916e5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing the \"best\" French cheese can be subjective, as it greatly depends on personal taste. France is renowned for its wide variety of cheeses, with over 400 different types. Here are a few highly regarded French cheeses across various categories:\n",
      "\n",
      "1. **Soft Cheeses**:\n",
      "   - **Brie de Meaux**: Known for its creamy texture and rich, slightly earthy flavor.\n",
      "   - **Camembert de Normandie**: Soft and creamy with a distinctive, pungent aroma.\n",
      "\n",
      "2. **Semi-Soft Cheeses**:\n",
      "   - **Munster**: A strong-smelling, wash-rind cheese with a powerful flavor, originating from the Alsace region.\n",
      "\n",
      "3. **Hard Cheeses**:\n",
      "   - **Comté**: A firm, nutty cheese made from unpasteurized cow's milk, often compared to Gruyère.\n",
      "   - **Beaufort**: Similar to Comté, with a firm texture and complex, nutty flavor.\n",
      "\n",
      "4. **Blue Cheeses**:\n",
      "   - **Roquefort**: A tangy, salty blue cheese made from sheep's milk, known for its distinctive veins of blue mold.\n",
      "\n",
      "5. **Goat Cheeses**:\n",
      "   - **Chèvre**: French goat cheese comes in many forms, such as fresh (Chèvre Frais) or aged (Chèvre Affiné). They often have a tangy, earthy flavor.\n",
      "\n",
      "Each of these cheeses has its unique characteristics, so the \"best\" one depends on your personal preferences. It's always fun to try several and decide which you like the most!\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.complete(\n",
    "    model= model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", # \"rôle\" que va jouer le robot (liste à choix fermée)\n",
    "            \"content\": \"What is the best French cheese?\", # la question\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ed256-b321-49a5-be6b-3e6f82014f1f",
   "metadata": {},
   "source": [
    "...et en français :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8b15cca-b672-4938-86c4-4899e7ee006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La question du \"meilleur fromage\" est très subjective et dépend des goûts personnels de chacun. La France, en particulier, est célèbre pour sa grande variété de fromages, chacun ayant ses propres caractéristiques et saveurs uniques. Voici quelques fromages français très appréciés :\n",
      "\n",
      "1. **Roquefort** : Un fromage persillé au lait de brebis, connu pour son goût fort et sa texture crémeuse.\n",
      "2. **Camembert** : Un fromage à pâte molle et à croûte fleurie, souvent apprécié pour sa douceur et son goût prononcé.\n",
      "3. **Brie de Meaux** : Un autre fromage à pâte molle et à croûte fleurie, célèbre pour sa saveur riche et crémeuse.\n",
      "4. **Comté** : Un fromage à pâte pressée cuite, au goût fruité et noiseté, qui peut varier en fonction de son âge.\n",
      "5. **Reblochon** : Un fromage à pâte pressée non cuite, souvent utilisé dans la tartiflette, avec un goût doux et crémeux.\n",
      "\n",
      "Il existe des centaines de fromages différents, et chacun a ses préférences. Le meilleur fromage est donc celui que vous appréciez le plus !\n"
     ]
    }
   ],
   "source": [
    "chat_response_fr = client.chat.complete(\n",
    "    model= model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"La réponse s'adresse à un locuteur français. Quel est le meilleur fromage ?\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(chat_response_fr.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a791b5-e212-4d2c-b66c-de8b33fa5090",
   "metadata": {},
   "source": [
    "On peut aussi demander à un modèle proche de fournir les plongements (*embeddings*) de textes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f959bc2-6763-4c75-ae22-4f7843f3e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = \"mistral-embed\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "embeddings_response = client.embeddings.create(\n",
    "    model=model_emb,\n",
    "    inputs=[\"Un fromage à pâte pressée cuite, originaire du Jura et de la Franche-Comté.\",\n",
    "            \"Il existe de nombreuses variétés de fromages de chèvre, comme le Crottin de Chavignol ou le Sainte-Maure de Tourain.\",\n",
    "           \"Le pithiviers est un gâteau français à base de pâte feuilletée originaire de la commune de Pithiviers située dans le département du Loiret et la région Centre-Val de Loire.\",\n",
    "           \"Le château de Versailles est un château et un monument historique français situé à Versailles, dans les Yvelines.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ef6a878-8d96-4548-b3f6-21e7beceb6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "vectors = [np.array(e.embedding) for e in embeddings_response.data]\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "256016c9-81cc-46fb-9d0c-a440094b2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarité cosinus entre deux vecteurs sous numpy\n",
    "def cosine(x, y):\n",
    "    return float(cosine_similarity(x.reshape(1, -1), y.reshape(1, -1)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58722edb-6fe2-4ec1-868b-87b5124bf615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité cosinus : 0.830 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarité cosinus : {cosine(vectors[0], vectors[1]):.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a05f3-bcd4-4164-9ebe-355ca0b93329",
   "metadata": {},
   "source": [
    "# Combiner Mistral avec du RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1973e143-650b-4a52-bb52-b104910aac45",
   "metadata": {},
   "source": [
    "Le RAG (*Retrieval Augmented Generation*) permet de combiner la recherche d'information (pour trouver les meilleurs passages) et la génération d'une réponse.\n",
    "\n",
    "La solution présentée ici ne recourt pas à des librairies comme Langchain ou LlamaIndex, que nous ne verrons pas en cours, mais beaucoup de ressources en ligne en parlent très bien (comme : https://docs.mistral.ai/guides/rag/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756af9a5-3beb-471a-aab9-85da39b692eb",
   "metadata": {},
   "source": [
    "Chargement des données textuelles à partir d'un fichier (cf. début du cours) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5801ce8c-1141-45fe-8475-bc9dbdebcc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5777 lignes dans le fichier texte\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\") as f:    \n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"{len(lines)} lignes dans le fichier texte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffd833-ecc7-4f73-b601-e2ada56b294b",
   "metadata": {},
   "source": [
    "On combine les lignes pour produire un corpus de paragraphes (il faut éviter les documents vides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbee879e-6c04-46b3-b292-ee25d71ca567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033 paragraphes\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "s = \"\"\n",
    "for l in lines:\n",
    "    if l:\n",
    "        s += \" \" + l\n",
    "    elif s:\n",
    "        docs.append(s)\n",
    "        s = \"\"\n",
    "\n",
    "print(f\"{len(docs)} paragraphes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ad50a-47c1-40a0-88a5-387e27079535",
   "metadata": {},
   "source": [
    "On choisit le modèle d'embedding et on initialise le modèle (comme tout à l'heure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbd89c98-c49c-408f-aff9-fcbf1930d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = \"mistral-embed\"\n",
    "\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bac49-ee24-4c50-9479-0cc3a4abe65b",
   "metadata": {},
   "source": [
    "Si la base de données vectorielle n'a pas été créée, il faut le faire (cela peut prendre du temps). Attribuer alors valeur *True* à la variable *build_embeddings* pour lancer le travail avec l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1170e5ef-2987-4d75-a601-4625883fec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_embeddings = False  # mettre True pour recalculer tous les plongements\n",
    "\n",
    "if build_embeddings:\n",
    "    text_embeddings = []\n",
    "    docs_2_process = docs.copy()\n",
    "    step = 512  # nombre de documents traités à la fois (à priori, max 512 pour Mistral large)\n",
    "\n",
    "    def process_docs(docs_batch):\n",
    "        embeddings_response = client.embeddings.create(\n",
    "            model=model_emb,\n",
    "            inputs=docs_batch\n",
    "        )\n",
    "        return [np.array(e.embedding) for e in embeddings_response.data]\n",
    "\n",
    "    while docs_2_process:\n",
    "        liste_docs = docs_2_process[:step]\n",
    "        text_embeddings.extend(process_docs(liste_docs))\n",
    "        docs_2_process = docs_2_process[step:]\n",
    "        print(len(liste_docs))\n",
    "        time.sleep(5)  # petit temps de latence pour ne pas surcharger l'API\n",
    "\n",
    "    vectors_list = np.array(text_embeddings)\n",
    "\n",
    "    with open(\"save_emb_alice.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vectors_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dcedd74-1220-426f-bb45-830855e370ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not build_embeddings:\n",
    "    with open(\"save_emb_alice.pkl\", \"rb\") as f:\n",
    "        vectors_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a80c80b1-c8d5-4400-b2ad-c95d1ca87b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2033, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(vectors_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from faiss import IndexFlatIP\n",
    "import faiss.contrib.torch_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d962e-1cf5-47ba-9851-f1c20d4920fa",
   "metadata": {},
   "source": [
    "A partir de là, on peut créer l'index de la base de données vectorielle grâce à la libaririe *faiss* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8b8ec1f-5ef6-4da7-a033-007713794bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 True\n"
     ]
    }
   ],
   "source": [
    "vectors_list_f32 = np.array(vectors_list, dtype=np.float32)\n",
    "\n",
    "dim = vectors_list_f32.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "print(dim, index.is_trained)\n",
    "index.add(vectors_list_f32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1aa21f-f5cf-4d76-8fcf-ddfad2b9697a",
   "metadata": {},
   "source": [
    "Note : pour accélérer les choses, le mieux serait de sauvegarder cet index sur le disque et pas de le recalculer à chaque fois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5279a89-3d49-431d-b52c-01e5a56d517f",
   "metadata": {},
   "source": [
    "# Poser des questions sur les données textuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee54a53-b7f9-479a-bcfb-7da6ba2ce9ad",
   "metadata": {},
   "source": [
    "A présent, on peut utiliser l'index pour poser n'importe quelle question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147896da-2e1d-4f41-8cb6-38edbf5205b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poser_une_question(question):\n",
    "    def create_embedding(input_text):\n",
    "        response = client.embeddings.create(\n",
    "            model=model_emb,\n",
    "            inputs=input_text\n",
    "        )\n",
    "        return np.array([response.data[0].embedding])\n",
    "\n",
    "    def retrieve_documents(embedding, k=2):\n",
    "        _, indices = index.search(embedding, k)\n",
    "        print(data)\n",
    "        print([docs[i] for i in indices.tolist()[0]])\n",
    "        return [docs[i] for i in indices.tolist()[0]]\n",
    "\n",
    "    question_embedding = create_embedding(question)\n",
    "    retrieved_docs = retrieve_documents(question_embedding)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Context information is below.\n",
    "            ---------------------\n",
    "                {retrieved_docs}\n",
    "            ---------------------\n",
    "            Given the context information and not prior knowledge, answer the query.\n",
    "                Query: {question}\n",
    "            Answer:\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c38591c-32a8-4271-ac75-febcdd84b3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33786166 0.3802628 ]]\n",
      "[\" [_She starts to open the door just as the DUCHESS comes out carrying a pig in baby's clothes. She sneezes--FROG sneezes and ALICE sneezes._]\", ' The Duchess!']\n",
      "A pig\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What unexpected animal the duchess comes out carrying in baby's clothes?\"\n",
    "print(poser_une_question(question1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64627427-0149-4aca-b960-775bec995c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the usual fate reserved for the queen's enemies is to be beheaded. This is indicated by the statement: \"You'd better not talk. I heard the Queen say only yesterday you deserved to be beheaded.\"\n"
     ]
    }
   ],
   "source": [
    "question2 = \"What is the usual fate reserved for the queen's enemies?\"\n",
    "print(poser_une_question(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b1d62-6817-4f2b-b528-59547b67c92f",
   "metadata": {},
   "source": [
    "Les réponses apportée semblent tout à fait pertinentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
