{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bae70d-0b59-43eb-a47a-bf71a002623b",
   "metadata": {
    "id": "81bae70d-0b59-43eb-a47a-bf71a002623b"
   },
   "source": [
    "# Mise en oeuvre du modèle Mistral Large (123B paramètres ?)\n",
    "\n",
    "Plus de détails sur le modèle : https://mistral.ai/fr/news/mistral-large/\n",
    "\n",
    "La partie RAG suit le traitement proposé ici : https://docs.mistral.ai/guides/rag/\n",
    "\n",
    "Il est possible d'affiner (fine tune) ce type de modèle, y compris via l'API si on n'a pas assez de ressources de calcul en local, mais ce ne sera pas vu dans ce cours : https://docs.mistral.ai/guides/finetuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71de07-ff3d-4da4-b056-3113040d17d9",
   "metadata": {
    "id": "9f71de07-ff3d-4da4-b056-3113040d17d9"
   },
   "source": [
    "Ce qui suit est une démonstration de l'utilisation de l'API de Mistral, comme alternative aux modèles de référence comme GPT. A noter qu'il est possible, en plus de faire de la génération à partir de prompt et des données de préentrainement du modèle, de suivre le principe RAG (Retrieval Augmented Generation) afin de coupler ça avec une base de données qui a été indexée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7gV1OMFqcOgT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7gV1OMFqcOgT",
    "outputId": "33f82429-b6c7-40c1-e060-11b5c5631faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !pip install mistralai faiss-gpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a86e29f-5e89-4f62-8bea-d8ec6093ab46",
   "metadata": {
    "id": "3a86e29f-5e89-4f62-8bea-d8ec6093ab46"
   },
   "outputs": [],
   "source": [
    "# librairies utilisées\n",
    "# !pip install mistralai faiss-gpu # if you use GPU\n",
    "# !pip install mistralai faiss-cpu # if you use CPU\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import faiss\n",
    "import pickle\n",
    "import faiss.contrib.torch_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf64b44-9115-47c1-a505-437ae410ddfe",
   "metadata": {
    "id": "5cf64b44-9115-47c1-a505-437ae410ddfe"
   },
   "source": [
    "Tout d'abord, il s'agit de charger la clef d'accès à l'API de Mistral (gratuit pour une utilisation individuelle mais potentiellement limitée)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc85de4-23c7-47cc-9b26-d46016dcd8c9",
   "metadata": {
    "id": "dbc85de4-23c7-47cc-9b26-d46016dcd8c9"
   },
   "outputs": [],
   "source": [
    "api_key = \"ZFQewleNy8baNFwwihwYBduUYJW7c7IF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fef28-a4cc-4687-b3a6-b5f178b488b9",
   "metadata": {
    "id": "200fef28-a4cc-4687-b3a6-b5f178b488b9"
   },
   "source": [
    "Après avoir [installé la librairie *mistralai*](https://docs.mistral.ai/getting-started/clients/), vous pouvez la charger en mémoire et choisir votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a11d85-df8c-4806-89a2-cbf9153a50b9",
   "metadata": {
    "id": "19a11d85-df8c-4806-89a2-cbf9153a50b9"
   },
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "# choix du modèle (ici la version large)\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61105d84-a57e-4c99-a4ef-1787eb40e5d8",
   "metadata": {
    "id": "61105d84-a57e-4c99-a4ef-1787eb40e5d8"
   },
   "source": [
    "Pour commencer, quelques tests simples d'autocomplétion, en anglais..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38054bc1-4921-481e-9fcc-f9ad916e5d54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38054bc1-4921-481e-9fcc-f9ad916e5d54",
    "outputId": "881b5db7-0ea8-4f9b-8c53-900f7f1e37f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" French cheese can be subjective, as it depends on personal taste. France is renowned for its wide variety of cheeses, with over 400 different types. Here are a few highly regarded French cheeses across various categories:\n",
      "\n",
      "1. **Soft Cheeses**:\n",
      "   - **Brie de Meaux**: Known for its creamy texture and rich, slightly earthy flavor.\n",
      "   - **Camembert de Normandie**: Soft and creamy with a strong, distinct aroma.\n",
      "\n",
      "2. **Semi-Soft Cheeses**:\n",
      "   - **Munster**: A strong-smelling, full-flavored cheese from the Alsace region.\n",
      "   - **Reblochon**: A nutty and fruity cheese from the Alps, often used in tartiflette.\n",
      "\n",
      "3. **Hard Cheeses**:\n",
      "   - **Comté**: A complex, nutty, and slightly sweet cheese made from unpasteurized cow's milk.\n",
      "   - **Beaufort**: Similar to Comté, with a firm texture and a sweet, nutty flavor.\n",
      "\n",
      "4. **Blue Cheeses**:\n",
      "   - **Roquefort**: A tangy and salty sheep milk cheese with distinctive veins of blue mold.\n",
      "   - **Bleu d'Auvergne**: A creamy and strong-flavored blue cheese from the Auvergne region.\n",
      "\n",
      "5. **Goat Cheeses**:\n",
      "   - **Chèvre**: French goat cheese comes in various shapes, sizes, and textures, ranging from mild to strong in flavor.\n",
      "   - **Crottin de Chavignol**: A small, round goat cheese with a distinct, strong flavor.\n",
      "\n",
      "Each of these cheeses has its unique characteristics, so the \"best\" one depends on your preferences. It's always enjoyable to try a variety to discover your favorite!\n"
     ]
    }
   ],
   "source": [
    "chat_response = client.chat.complete(\n",
    "    model= model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", # \"rôle\" que va jouer le robot (liste à choix fermée)\n",
    "            \"content\": \"What is the best French cheese?\", # la question\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ed256-b321-49a5-be6b-3e6f82014f1f",
   "metadata": {
    "id": "b66ed256-b321-49a5-be6b-3e6f82014f1f"
   },
   "source": [
    "...et en français :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b15cca-b672-4938-86c4-4899e7ee006a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8b15cca-b672-4938-86c4-4899e7ee006a",
    "outputId": "44986b96-85d9-4a21-dd84-e64d2f76d203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La question du \"meilleur fromage\" est très subjective et dépend des goûts personnels, car il existe une grande variété de fromages avec des saveurs, des textures et des arômes très différents. Cependant, certains fromages sont souvent cités parmi les plus célèbres et appréciés en France et dans le monde. Voici quelques exemples :\n",
      "\n",
      "1. **Roquefort** : Un fromage de brebis bleu, connu pour son goût intense et sa texture crémeuse.\n",
      "2. **Camembert** : Un fromage à pâte molle et à croûte fleurie, originaire de Normandie.\n",
      "3. **Brie** : Un autre fromage à pâte molle et à croûte fleurie, originaire de la région de Brie.\n",
      "4. **Comté** : Un fromage à pâte pressée cuite, produit dans le Jura, avec une saveur riche et complexe.\n",
      "5. **Reblochon** : Un fromage à pâte pressée non cuite, originaire de Savoie, souvent utilisé dans la recette de la tartiflette.\n",
      "6. **Chèvre** : Il existe de nombreuses variétés de fromages de chèvre, avec des saveurs allant du doux au corsé.\n",
      "\n",
      "Chaque fromage a ses propres caractéristiques et peut être apprécié dans différents contextes, que ce soit en apéritif, en plat principal ou en dessert. Le \"meilleur\" fromage est donc celui qui correspond le mieux à vos préférences personnelles en matière de goût et de texture.\n"
     ]
    }
   ],
   "source": [
    "chat_response_fr = client.chat.complete(\n",
    "    model= model,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"La réponse s'adresse à un locuteur français. Quel est le meilleur fromage ?\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print(chat_response_fr.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a791b5-e212-4d2c-b66c-de8b33fa5090",
   "metadata": {
    "id": "02a791b5-e212-4d2c-b66c-de8b33fa5090"
   },
   "source": [
    "On peut aussi demander à un modèle proche de fournir les plongements (*embeddings*) de textes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f959bc2-6763-4c75-ae22-4f7843f3e9b7",
   "metadata": {
    "id": "1f959bc2-6763-4c75-ae22-4f7843f3e9b7"
   },
   "outputs": [],
   "source": [
    "model_emb = \"mistral-embed\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "embeddings_response = client.embeddings.create(\n",
    "    model=model_emb,\n",
    "    inputs=[\"Un fromage à pâte pressée cuite, originaire du Jura et de la Franche-Comté.\",\n",
    "            \"Il existe de nombreuses variétés de fromages de chèvre, comme le Crottin de Chavignol ou le Sainte-Maure de Tourain.\",\n",
    "           \"Le pithiviers est un gâteau français à base de pâte feuilletée originaire de la commune de Pithiviers située dans le département du Loiret et la région Centre-Val de Loire.\",\n",
    "           \"Le château de Versailles est un château et un monument historique français situé à Versailles, dans les Yvelines.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef6a878-8d96-4548-b3f6-21e7beceb6f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ef6a878-8d96-4548-b3f6-21e7beceb6f3",
    "outputId": "b7c9302b-7d26-404c-865c-8c9f3f73b1c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "vectors = [np.array(e.embedding) for e in embeddings_response.data]\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256016c9-81cc-46fb-9d0c-a440094b2fba",
   "metadata": {
    "id": "256016c9-81cc-46fb-9d0c-a440094b2fba"
   },
   "outputs": [],
   "source": [
    "# similarité cosinus entre deux vecteurs sous numpy\n",
    "def cosine(x, y):\n",
    "    return float(cosine_similarity(x.reshape(1, -1), y.reshape(1, -1)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58722edb-6fe2-4ec1-868b-87b5124bf615",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58722edb-6fe2-4ec1-868b-87b5124bf615",
    "outputId": "f4043176-3f95-441a-99a0-091df4436875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité cosinus : 0.830 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarité cosinus : {cosine(vectors[0], vectors[1]):.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a05f3-bcd4-4164-9ebe-355ca0b93329",
   "metadata": {
    "id": "251a05f3-bcd4-4164-9ebe-355ca0b93329"
   },
   "source": [
    "# Combiner Mistral avec du RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1973e143-650b-4a52-bb52-b104910aac45",
   "metadata": {
    "id": "1973e143-650b-4a52-bb52-b104910aac45"
   },
   "source": [
    "Le RAG (*Retrieval Augmented Generation*) permet de combiner la recherche d'information (pour trouver les meilleurs passages) et la génération d'une réponse.\n",
    "\n",
    "La solution présentée ici ne recourt pas à des librairies comme Langchain ou LlamaIndex, que nous ne verrons pas en cours, mais beaucoup de ressources en ligne en parlent très bien (comme : https://docs.mistral.ai/guides/rag/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756af9a5-3beb-471a-aab9-85da39b692eb",
   "metadata": {
    "id": "756af9a5-3beb-471a-aab9-85da39b692eb"
   },
   "source": [
    "Chargement des données textuelles à partir d'un fichier (cf. début du cours) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "x00tDdktcnnh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x00tDdktcnnh",
    "outputId": "dda3765c-038c-48ad-d199-7de1d301c37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-06 12:28:22--  https://raw.githubusercontent.com/upunaprosk/ul2-nlp-course/refs/heads/2024/TD4-mistral-rag/alice.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 104081 (102K) [text/plain]\n",
      "Saving to: ‘alice.txt’\n",
      "\n",
      "alice.txt           100%[===================>] 101.64K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-11-06 12:28:23 (921 KB/s) - ‘alice.txt’ saved [104081/104081]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/upunaprosk/ul2-nlp-course/refs/heads/2024/TD4-mistral-rag/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5801ce8c-1141-45fe-8475-bc9dbdebcc5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5801ce8c-1141-45fe-8475-bc9dbdebcc5d",
    "outputId": "8221b45b-48e6-4cda-859f-c8453ec979e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5777 lignes dans le fichier texte\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\") as f:\n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"{len(lines)} lignes dans le fichier texte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffd833-ecc7-4f73-b601-e2ada56b294b",
   "metadata": {
    "id": "73ffd833-ecc7-4f73-b601-e2ada56b294b"
   },
   "source": [
    "On combine les lignes pour produire un corpus de paragraphes (il faut éviter les documents vides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbee879e-6c04-46b3-b292-ee25d71ca567",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbee879e-6c04-46b3-b292-ee25d71ca567",
    "outputId": "4915cac4-7eb8-48de-fc84-412d71b99bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033 paragraphes\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "s = \"\"\n",
    "for l in lines:\n",
    "    if (l != \"\"):\n",
    "        s = s + \" \" + l\n",
    "    else:\n",
    "        if (s != \"\"):\n",
    "            docs = docs + [s]\n",
    "            s = \"\"\n",
    "\n",
    "print(f\"{len(docs)} paragraphes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ad50a-47c1-40a0-88a5-387e27079535",
   "metadata": {
    "id": "657ad50a-47c1-40a0-88a5-387e27079535"
   },
   "source": [
    "On choisit le modèle d'embedding et on initialise le modèle (comme tout à l'heure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbd89c98-c49c-408f-aff9-fcbf1930d59a",
   "metadata": {
    "id": "fbd89c98-c49c-408f-aff9-fcbf1930d59a"
   },
   "outputs": [],
   "source": [
    "model_emb = \"mistral-embed\"\n",
    "\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bac49-ee24-4c50-9479-0cc3a4abe65b",
   "metadata": {
    "id": "fe1bac49-ee24-4c50-9479-0cc3a4abe65b"
   },
   "source": [
    "Si la base de données vectorielle n'a pas été créée, il faut le faire (cela peut prendre du temps). Attribuer alors valeur *True* à la variable *build_embeddings* pour lancer le travail avec l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1170e5ef-2987-4d75-a601-4625883fec43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1170e5ef-2987-4d75-a601-4625883fec43",
    "outputId": "53f5e6f8-760c-491d-e3aa-a507ebabdb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n",
      "512\n",
      "497\n"
     ]
    }
   ],
   "source": [
    "build_embeddings = True # mettre True pour recalculer tous les plongements\n",
    "\n",
    "if build_embeddings:\n",
    "\n",
    "    text_embeddings = []\n",
    "    docs_2processed = docs.copy()\n",
    "\n",
    "    step = 512 # nombre de documents traités à la fois (à priori, max 512 pour Mistral large)\n",
    "\n",
    "    while (len(docs_2processed)>step):\n",
    "        # liste des documents à traiter\n",
    "        liste_docs = docs_2processed[:step]\n",
    "\n",
    "        #on demande à l'API de calculer les vecteurs\n",
    "        embeddings_response = client.embeddings.create(\n",
    "            model=model_emb,\n",
    "            inputs = liste_docs\n",
    "        )\n",
    "\n",
    "        # on ajoute à la liste des vecteurs déjà calculés les nouveaux plongeements\n",
    "        text_embeddings.append([np.array(e.embedding) for e in embeddings_response.data])\n",
    "\n",
    "        # on décale le \"curseur\" sur la liste de traitement\n",
    "        docs_2processed = docs_2processed[step:]\n",
    "        print(len(liste_docs))\n",
    "\n",
    "        # petit temps de latence pour ne pas surcharger l'API\n",
    "        time.sleep(5)\n",
    "\n",
    "    # même traitement mais pour les derniers documents (liste de longueur < step)\n",
    "    if (len(docs_2processed)>0):\n",
    "        liste_docs = docs_2processed\n",
    "        embeddings_response = client.embeddings.create(\n",
    "            model=model_emb,\n",
    "            inputs = liste_docs\n",
    "        )\n",
    "        print(len(liste_docs))\n",
    "        text_embeddings.append([np.array(e.embedding) for e in embeddings_response.data])\n",
    "\n",
    "    # On construit une liste de tableau format numpy à partir de la liste des tableaux (c'est une simple concaténation)\n",
    "    vectors_list = np.array([j for i in text_embeddings for j in i] )\n",
    "\n",
    "    # On pense à sauvegarder le tableau de plongement dans un fichier sur le disque\n",
    "    with open(\"save_emb_alice.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vectors_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "p_NQZl72c2X7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_NQZl72c2X7",
    "outputId": "b6e1311d-f22a-47fc-8eaf-a97256c05046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice.txt  sample_data\tsave_emb_alice.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dcedd74-1220-426f-bb45-830855e370ac",
   "metadata": {
    "id": "6dcedd74-1220-426f-bb45-830855e370ac"
   },
   "outputs": [],
   "source": [
    "if not build_embeddings:\n",
    "    with open(\"save_emb_alice.pkl\", \"rb\") as f:\n",
    "        vectors_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a80c80b1-c8d5-4400-b2ad-c95d1ca87b2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a80c80b1-c8d5-4400-b2ad-c95d1ca87b2c",
    "outputId": "0fdb047e-8f71-4bcf-b594-4ad7e1611b22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2033, 1024)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d962e-1cf5-47ba-9851-f1c20d4920fa",
   "metadata": {
    "id": "d33d962e-1cf5-47ba-9851-f1c20d4920fa"
   },
   "source": [
    "A partir de là, on peut créer l'index de la base de données vectorielle grâce à la libaririe *faiss* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8b8ec1f-5ef6-4da7-a033-007713794bd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8b8ec1f-5ef6-4da7-a033-007713794bd2",
    "outputId": "847945e2-edf5-401f-8293-b5204494a257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "vectors_list_f32 = np.array(vectors_list).astype('float32')\n",
    "d = vectors_list_f32.shape[1]\n",
    "print(d)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(index.is_trained)\n",
    "index.add(vectors_list_f32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1aa21f-f5cf-4d76-8fcf-ddfad2b9697a",
   "metadata": {
    "id": "5b1aa21f-f5cf-4d76-8fcf-ddfad2b9697a"
   },
   "source": [
    "Note : pour accélérer les choses, le mieux serait de sauvegarder cet index sur le disque et pas de le recalculer à chaque fois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5279a89-3d49-431d-b52c-01e5a56d517f",
   "metadata": {
    "id": "e5279a89-3d49-431d-b52c-01e5a56d517f"
   },
   "source": [
    "# Poser des questions sur les données textuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee54a53-b7f9-479a-bcfb-7da6ba2ce9ad",
   "metadata": {
    "id": "2ee54a53-b7f9-479a-bcfb-7da6ba2ce9ad"
   },
   "source": [
    "A présent, on peut utiliser l'index pour poser n'importe quelle question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "147896da-2e1d-4f41-8cb6-38edbf5205b7",
   "metadata": {
    "id": "147896da-2e1d-4f41-8cb6-38edbf5205b7"
   },
   "outputs": [],
   "source": [
    "def poser_une_question(question):\n",
    "\n",
    "    # création du plongement (=vecteur) correspondant à la question posée\n",
    "    embeddings_response = client.embeddings.create(\n",
    "        model=model_emb,\n",
    "        inputs = question\n",
    "    )\n",
    "    question_embedding = np.array([embeddings_response.data[0].embedding], dtype=np.float32)\n",
    "\n",
    "    # recherche d'information : on sélectionne les 2 documents ayant des vecteurs les plus similaires à la question (cf. TD1)\n",
    "    D, I = index.search(question_embedding, k=2)\n",
    "    retrieved_docs = [docs[i] for i in I.tolist()[0]]\n",
    "\n",
    "    # on crée l'invite (prompt) qui va nous permettre d'interroger le modèle de langue\n",
    "    prompt = f\"\"\"\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "            {retrieved_docs}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "            Query: {question}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    # ajout d'un petit temps de latence pour éviter de surcharger l'API\n",
    "    time.sleep(5)\n",
    "\n",
    "    # on lance la requête en ligne\n",
    "    chat_response = client.chat.complete(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c38591c-32a8-4271-ac75-febcdd84b3db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c38591c-32a8-4271-ac75-febcdd84b3db",
    "outputId": "cb5a4472-9c62-4146-c739-ae29853f0a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duchess comes out carrying a pig in baby's clothes.\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What unexpected animal the duchess comes out carrying in baby's clothes?\"\n",
    "question2 = \"What is the usual fate reserved for the queen's enemies?\"\n",
    "\n",
    "print(poser_une_question(question1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64627427-0149-4aca-b960-775bec995c7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64627427-0149-4aca-b960-775bec995c7e",
    "outputId": "cee3a965-fa91-40ea-fed1-2369c5296ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context information, the usual fate reserved for the queen's enemies is to be beheaded.\n"
     ]
    }
   ],
   "source": [
    "print(poser_une_question(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b1d62-6817-4f2b-b528-59547b67c92f",
   "metadata": {
    "id": "ba6b1d62-6817-4f2b-b528-59547b67c92f"
   },
   "source": [
    "Les réponses apportée semblent tout à fait pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d09819b-83e5-4611-b665-01e30b629c65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "4e10e4547c9d41ea827cae45027bc9c9",
      "050f6a1b23844530bb36fe5c7f201fb3",
      "a25cbf86febb49779751215b652e473d",
      "077ccaab2e3a4e9190183b847e6defb5",
      "efbf29d1100f45fb9983f0e0508c2dcf",
      "cb7868a470464150876825c5151d1149",
      "188a52e4e441435ba07893db8f6ac2b4",
      "9ddd84ee90ef494b8ec73ce1067c5ee6",
      "01f0c716e5a34abfa06dee9091c10edd",
      "6bdb0d8a464146d9abca820378bd212e"
     ]
    },
    "id": "9d09819b-83e5-4611-b665-01e30b629c65",
    "outputId": "ea288a48-6109-42f6-80c9-7955dca6d758"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e10e4547c9d41ea827cae45027bc9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Query:', layout=Layout(width='80%'), placeholder='Type your query h…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f0c716e5a34abfa06dee9091c10edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "query_input = widgets.Text(\n",
    "    placeholder=\"Type your query here...\",\n",
    "    description=\"Query:\",\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Submit\",\n",
    "    button_style=\"success\",\n",
    "    layout=widgets.Layout(width='20%')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_submit_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        query = query_input.value\n",
    "        if query.strip():\n",
    "            try:\n",
    "\n",
    "                response = poser_une_question(query)\n",
    "                print(\"Response:\", response)\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "        else:\n",
    "            print(\"Please enter a query.\")\n",
    "\n",
    "\n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "display(widgets.HBox([query_input, submit_button]), output_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SovaadQeeS2C",
   "metadata": {
    "id": "SovaadQeeS2C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP-Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01f0c716e5a34abfa06dee9091c10edd": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_6bdb0d8a464146d9abca820378bd212e",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Response: Alice sees nothing but odd black lanterns with orange lights, hanging, presumably, from the sky.\n"
        ]
       }
      ]
     }
    },
    "050f6a1b23844530bb36fe5c7f201fb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "Query:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_efbf29d1100f45fb9983f0e0508c2dcf",
      "placeholder": "Type your query here...",
      "style": "IPY_MODEL_cb7868a470464150876825c5151d1149",
      "value": "What does Alice see when she first falls down the rabbit hole?"
     }
    },
    "077ccaab2e3a4e9190183b847e6defb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "188a52e4e441435ba07893db8f6ac2b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20%"
     }
    },
    "4e10e4547c9d41ea827cae45027bc9c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_050f6a1b23844530bb36fe5c7f201fb3",
       "IPY_MODEL_a25cbf86febb49779751215b652e473d"
      ],
      "layout": "IPY_MODEL_077ccaab2e3a4e9190183b847e6defb5"
     }
    },
    "6bdb0d8a464146d9abca820378bd212e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ddd84ee90ef494b8ec73ce1067c5ee6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "a25cbf86febb49779751215b652e473d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "success",
      "description": "Submit",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_188a52e4e441435ba07893db8f6ac2b4",
      "style": "IPY_MODEL_9ddd84ee90ef494b8ec73ce1067c5ee6",
      "tooltip": ""
     }
    },
    "cb7868a470464150876825c5151d1149": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial"
     }
    },
    "efbf29d1100f45fb9983f0e0508c2dcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "80%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
