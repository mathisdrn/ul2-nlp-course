{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28552bb-f089-4780-b7d0-6e824e74bf45",
   "metadata": {},
   "source": [
    "# Mise en place d'une solution locale pour traitement par un LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6080f9-52f2-455f-a2db-f245ecb8339e",
   "metadata": {},
   "source": [
    "Cette solution se base sur [ollama](https://ollama.com). Il est nécessaire d'installer l'application qui va aussi installer la ligne de commande *ollama*, puis la librairie Python du même nom via *pip*.\n",
    "\n",
    "Dans l'exemple ci-dessous, nous allons utiliser l'un des modèles proposés : llama3.1. Il faut donc le télécharger localement sur votre machine via la commande suivante dans un terminal : *ollama pull llama3.1*\n",
    "\n",
    "Pour l'encodage du sens des documents, il faut installer la librairie *sentence_transformers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f85157bf-03f8-4740-843b-3ad2b9b517de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On importe les librairies utiles\n",
    "# !pip install ollama faiss-gpu # if you use GPU\n",
    "# !pip install ollama faiss-cpu # if you use CPU\n",
    "import ollama\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8575bf7-ca75-41be-ad7f-29dcbb534977",
   "metadata": {},
   "source": [
    "Pour travailler uniquement en local, on a besoin d'un modèle de type *Sentence Transformer* pour plonger les documents. L'un des plus performants aujourd'hui est peut-être *bget-small-en* et il en existe spécialisés sur le français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7628e919-11c1-42b5-8112-5fc01a73c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  SentenceTransformer(\"ggrn/bge-small-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c18aa-04f9-4515-9b71-fb906b52a668",
   "metadata": {},
   "source": [
    "Chargement du corpus, comme d'habitude mais en imposant une taille minimum aux documents/paragraphes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb099a28-2cd3-423d-b1a1-29ad9df1544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5777 lignes dans le fichier texte\n",
      "916 paragraphes\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"data\", \"alice.txt\")) as f:    \n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"{len(lines)} lignes dans le fichier texte\")\n",
    "\n",
    "docs = []\n",
    "s = \"\"\n",
    "for l in lines:\n",
    "    if (l != \"\"):\n",
    "        s = s + \" \" + l\n",
    "    else:\n",
    "        if (s != \"\"):\n",
    "            if len(s.split(\" \"))>5: # ici, il faut au moins 5 mots dans le paragraphe\n",
    "                docs = docs + [s]\n",
    "            s = \"\"\n",
    "\n",
    "print(f\"{len(docs)} paragraphes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8ebe60-7067-41f7-9b89-2d58836e115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_embedding = False # mettre True la première fois pour calculer et sauvegarder les plongements\n",
    "\n",
    "if build_embedding:\n",
    "    \n",
    "    steps = 200 # nombre de documents traités à la fois\n",
    "    \n",
    "    embeddings_full = np.zeros((len(docs), 384), dtype=np.float32) # 384 est la taille des plongements\n",
    "\n",
    "    num_batches = math.floor(len(docs)/steps)\n",
    "    for batch_num in tqdm(range(num_batches)):\n",
    "\n",
    "        embeddings = model.encode(docs[batch_num*steps:(batch_num+1)*steps], show_progress_bar=True)\n",
    "        embeddings_full[batch_num*steps:(batch_num+1)*steps] = embeddings\n",
    "    \n",
    "    embeddings = model.encode(docs[num_batches*steps:])\n",
    "    embeddings_full[num_batches*steps:] = embeddings\n",
    "    np.save(\"emb_alice\", embeddings_full)\n",
    "    \n",
    "else:\n",
    "    embeddings_full = np.load(\"emb_alice.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c50946b3-67e3-4f9a-817e-5dc90faa22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# A noter que l'index pourrait être, lui aussi, sauvegardé pour éviter de ré-indexer à chaque fois\n",
    "\n",
    "d = embeddings_full.shape[1]\n",
    "print(d)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(index.is_trained)\n",
    "index.add(embeddings_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f29d38-f016-4811-ab7b-836c0570e552",
   "metadata": {},
   "source": [
    "On construit une fonction qui prend en entrée la question posée puis réalise tout le traitement, à savoir :\n",
    "\n",
    "- plongement de la question = vecteur encodant le sens de la question (*embedding*)\n",
    "- recherche dans l'index en précisant le nombre de documents retournés (paramètre k)\n",
    "- mis au point de l'invite (*prompt*) en utilisant les documents retrouvés et la question\n",
    "- interrogation du LLM via *ollama*\n",
    "- retourne la réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39d576b1-a386-4650-8b8b-c177610405d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poser_une_question(question):\n",
    "    question_embedding = np.array([model.encode(question)])\n",
    "    D, I = index.search(question_embedding, k=10)\n",
    "    retrieved_docs = [docs[i] for i in I.tolist()[0]]\n",
    "    #print(retrieved_docs) # à décommenter si vous voulez afficher les documents retrouvés\n",
    "    prompt = f\"\"\"\n",
    "            Context information is below.\n",
    "            ---------------------\n",
    "                {retrieved_docs}\n",
    "            ---------------------\n",
    "            Given the context information and not prior knowledge, answer the query.\n",
    "                Query: {question}\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    response = ollama.chat(model='llama3.1', messages=\n",
    "            messages\n",
    "        )\n",
    "    return response['message']['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b8a38c1-59a2-462e-b878-9dd957df1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context information, the Duchess comes out carrying a pig in baby's clothes. This is an unexpected and surprising sight, as one would not typically associate pigs with being dressed like babies!\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What unexpected animal the duchess comes out carrying in baby's clothes?\"\n",
    "question2 = \"What is the usual fate reserved for the queen's enemies?\"\n",
    "question3 = \"Who carries the crown on a cushion?\"\n",
    "\n",
    "print(poser_une_question(question1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ea353-fb69-4ea0-b60d-bcc2838113b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
