{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28552bb-f089-4780-b7d0-6e824e74bf45",
   "metadata": {},
   "source": [
    "# Mise en place d'une solution locale pour traitement par un LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6080f9-52f2-455f-a2db-f245ecb8339e",
   "metadata": {},
   "source": [
    "Cette solution se base sur [ollama](https://ollama.com). Il est nécessaire d'installer l'application qui va aussi installer la ligne de commande *ollama*, puis la librairie Python du même nom via *pip*.\n",
    "\n",
    "Dans l'exemple ci-dessous, nous allons utiliser l'un des modèles proposés : llama3.1. Il faut donc le télécharger localement sur votre machine via la commande suivante dans un terminal : *ollama pull llama3.1*\n",
    "\n",
    "Pour l'encodage du sens des documents, il faut installer la librairie *sentence_transformers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85157bf-03f8-4740-843b-3ad2b9b517de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On importe les librairies utiles\n",
    "# !pip install ollama faiss-gpu # if you use GPU\n",
    "# !pip install ollama faiss-cpu # if you use CPU\n",
    "import ollama\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8575bf7-ca75-41be-ad7f-29dcbb534977",
   "metadata": {},
   "source": [
    "Pour travailler uniquement en local, on a besoin d'un modèle de type *Sentence Transformer* pour plonger les documents. L'un des plus performants aujourd'hui est peut-être *bget-small-en* et il en existe spécialisés sur le français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7628e919-11c1-42b5-8112-5fc01a73c633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdd44b6906a425993005e48c1a38976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef3b82201e84973aad87cba15cb678e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03d9f120bb347bd88e018922938f488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/17.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a9399c9bc74badbbc83a3b89125909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab72fc1621e648fb8f25b0444b7268c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8773054020cd4f69bd665b2b80fc577f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0e0628771346ca97dc951bdc94998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2027703645415986339e54f53ca6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf41d1203134bc2ba395702c867a620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5df0669b8f54b0fb3edabb84200c326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cae81d6c6f45159f49b0932967e6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model =  SentenceTransformer(\"ggrn/bge-small-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c18aa-04f9-4515-9b71-fb906b52a668",
   "metadata": {},
   "source": [
    "Chargement du corpus, comme d'habitude mais en imposant une taille minimum aux documents/paragraphes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb099a28-2cd3-423d-b1a1-29ad9df1544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5777 lignes dans le fichier texte\n",
      "916 paragraphes\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\") as f:    \n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"{len(lines)} lignes dans le fichier texte\")\n",
    "\n",
    "docs = []\n",
    "s = \"\"\n",
    "for l in lines:\n",
    "    if l:\n",
    "        s += \" \" + l\n",
    "    elif s:\n",
    "        if len(s.split(\" \")) > 5: # more than 5 words in text\n",
    "            docs.append(s)\n",
    "        s = \"\"\n",
    "\n",
    "print(f\"{len(docs)} paragraphes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8ebe60-7067-41f7-9b89-2d58836e115d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655652bd7921482daf25c10114edc871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e146712529cb416d8a672773510ee513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3da8b16b76a484390edc3297af4f0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9543e2e21238457fb5b9b0a2c26a1761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e26e8693414ecd9cd736500edd4727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_embedding = True # mettre True la première fois pour calculer et sauvegarder les plongements\n",
    "\n",
    "if build_embedding:\n",
    "    \n",
    "    steps = 200 # nombre de documents traités à la fois\n",
    "    \n",
    "    embeddings_full = np.zeros((len(docs), 384), dtype=np.float32) # 384 est la taille des plongements\n",
    "\n",
    "    num_batches = math.floor(len(docs)/steps)\n",
    "    for batch_num in tqdm(range(num_batches)):\n",
    "\n",
    "        embeddings = model.encode(docs[batch_num*steps:(batch_num+1)*steps], show_progress_bar=True)\n",
    "        embeddings_full[batch_num*steps:(batch_num+1)*steps] = embeddings\n",
    "    \n",
    "    embeddings = model.encode(docs[num_batches*steps:])\n",
    "    embeddings_full[num_batches*steps:] = embeddings\n",
    "    np.save(\"emb_alice\", embeddings_full)\n",
    "    \n",
    "else:\n",
    "    embeddings_full = np.load(\"emb_alice.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50946b3-67e3-4f9a-817e-5dc90faa22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 True\n"
     ]
    }
   ],
   "source": [
    "# A noter que l'index pourrait être, lui aussi, sauvegardé pour éviter de ré-indexer à chaque fois\n",
    "\n",
    "dim = embeddings_full.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "print(dim, index.is_trained)\n",
    "index.add(embeddings_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f29d38-f016-4811-ab7b-836c0570e552",
   "metadata": {},
   "source": [
    "On construit une fonction qui prend en entrée la question posée puis réalise tout le traitement, à savoir :\n",
    "\n",
    "- plongement de la question = vecteur encodant le sens de la question (*embedding*)\n",
    "- recherche dans l'index en précisant le nombre de documents retournés (paramètre k)\n",
    "- mis au point de l'invite (*prompt*) en utilisant les documents retrouvés et la question\n",
    "- interrogation du LLM via *ollama*\n",
    "- retourne la réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d576b1-a386-4650-8b8b-c177610405d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poser_une_question(question):\n",
    "    question_embedding = np.array([model.encode(question)])\n",
    "    _, indices = index.search(question_embedding, k=1)\n",
    "    retrieved_docs = docs[indices[0][0]]\n",
    "    #print(retrieved_docs) # à décommenter si vous voulez afficher les documents retrouvés\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "            Context information is below.\n",
    "            ---------------------\n",
    "                {retrieved_docs}\n",
    "            ---------------------\n",
    "            Given the context information and not prior knowledge, answer the query.\n",
    "                Query: {question}\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    response = ollama.chat(model='llama3.1', messages=\n",
    "            messages\n",
    "        )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8a38c1-59a2-462e-b878-9dd957df1ac7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m question2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the usual fate reserved for the queen\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms enemies?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m question3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho carries the crown on a cushion?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mposer_une_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion1\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mposer_une_question\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mposer_une_question\u001b[39m(question):\n\u001b[0;32m----> 2\u001b[0m     question_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([model\u001b[38;5;241m.\u001b[39mencode(question)])\n\u001b[1;32m      3\u001b[0m     _, indices \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(question_embedding, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     retrieved_docs \u001b[38;5;241m=\u001b[39m docs[indices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "question1 = \"What unexpected animal the duchess comes out carrying in baby's clothes?\"\n",
    "question2 = \"What is the usual fate reserved for the queen's enemies?\"\n",
    "question3 = \"Who carries the crown on a cushion?\"\n",
    "\n",
    "print(poser_une_question(question1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
