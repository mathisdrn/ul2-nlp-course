# CM2: Word Embeddings

[Slides used (PDF)](https://drive.google.com/file/d/1y2GKIKBzie7l8iycBO6gTKGiTTfJc4Dr/view?usp=sharing)

**Topics:**
- Vector representation of words. 
- Embeddings obtained with one-hot encoding. 
- Distributional hypothesis. 
- Word-word co-occurrence and PMI matrices. 
- Word-document matrices for tf-idf. 
- Overview of word2vec models.


**Links to theory and practice for computing tf-idf/PMI/word2vec:**

| Algorithm      | Implementation Example Link |
|----------------|----------------------------------------|
| TF-IDF         | [Tutorial](https://medium.com/@coldstart_coder/understanding-and-implementing-tf-idf-in-python-a325d1301484)    |
| PMI            | [Notebook](https://colab.research.google.com/drive/1QRhqKopX0nnpY6302lS2oJII5LTEiiT5?usp=sharing)     |
| Word2Vec       | [Notebook](https://github.com/dcavar/python-tutorial-notebooks/blob/master/notebooks/Word2Vec.ipynb)   |

- Lecture video from Stanford CS224N - [**Intro**](https://www.youtube.com/watch?v=OQQ-W_63UgQ), [**Embeddings**](https://www.youtube.com/watch?v=ERibwqs9p38)

### Fun with Embeddings
![embedding_space_walk](https://github.com/yandexdataschool/nlp_course/blob/2024/resources/nlp2020_gifs/walk_through_space.gif)

Link to try yourself: [LINK](https://lena-voita.github.io/nlp_course/word_embeddings.html#analysis_interpretability).
