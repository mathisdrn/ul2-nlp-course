The exam date was set for the 5th of December from 9 to 11 am.
To prepare, you can revise the topics we covered:

1) Introduction to Natural Language Processing (NLP). Challenges of text processing (word ambiguity, idioms, slang, misspelling). Existing applications of NLP (translation, trend analysis, summarization, virtual assistants). Text preprocessing steps. Lemmatization vs stemming. (CM 1) 
2) Vector representation of words. Embeddings obtained with one-hot encoding. Distributional hypothesis. Word-word co-occurrence and PMI matrices. Word-document matrices for tf-idf. Overview of word2vec models. (CM2)
3) Summary of approaches to vector representation. Negative sampling. Word2Vec: skip-gram vs CBOW. Linear operations with vectors, including addition and subtraction. Impact of large/small context window size on embedding results. Problem statement for text classification. Overview of feature extraction approaches: count-based vs neural. Overview of text classification with Naive Bayes. (CM3)
4) Overview of feature extraction approaches: count-based vs neural. Text classification with Naive Bayes. Laplace (add-one) smoothing. Text classification with Logistic Regression. Training: Maximizing Likelihood. Na√Øve Bayes vs Logistic Regression. Text classification with SVM. Overview of classification with Neural Networks. Data Augmentation for Text. (CM4) 
5) Aurora-sentence-embeddings (CM5)
6) Neural Networks. Fully-connected neural networks. Activation functions. Forward pass and backward pass. Attention Mechanism.  (CM6)

All materials can be found here: https://github.com/upunaprosk/ul2-nlp-course/tree/2024?tab=readme-ov-file#course-plan